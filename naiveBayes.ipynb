{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20be6480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db0341",
   "metadata": {},
   "source": [
    "## 1. TF (Term Frequency) et TF-IDF\n",
    "\n",
    "### 2.1 Concept de TF (Term Frequency)\n",
    "\n",
    "**Définition** : La fréquence d'un terme dans un document.\n",
    "\n",
    "**Formule** : TF(t, d) = (Nombre de fois où le terme t apparaît dans le document d) / (Nombre total de termes dans d)\n",
    "\n",
    "### Exemple simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "034c7f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document : le chat mange la souris\n",
      "Nombre total de mots : 5\n",
      "\n",
      "Fréquences brutes : Counter({'le': 1, 'chat': 1, 'mange': 1, 'la': 1, 'souris': 1})\n",
      "\n",
      "TF normalisé : {'le': 0.2, 'chat': 0.2, 'mange': 0.2, 'la': 0.2, 'souris': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Corpus de documents\n",
    "documents = [\n",
    "    \"le chat mange la souris\",\n",
    "    \"le chien mange les croquettes\",\n",
    "    \"la souris est petite\"\n",
    "]\n",
    "\n",
    "# Calcul manuel du TF\n",
    "doc1 = \"le chat mange la souris\"\n",
    "mots = doc1.split()\n",
    "print(f\"Document : {doc1}\")\n",
    "print(f\"Nombre total de mots : {len(mots)}\")\n",
    "\n",
    "# Compter chaque mot\n",
    "from collections import Counter\n",
    "compteur = Counter(mots)\n",
    "print(f\"\\nFréquences brutes : {compteur}\")\n",
    "\n",
    "# TF normalisé\n",
    "tf_doc1 = {mot: count/len(mots) for mot, count in compteur.items()}\n",
    "print(f\"\\nTF normalisé : {tf_doc1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7354ba01",
   "metadata": {},
   "source": [
    "### 1.2 TF-IDF (Term Frequency - Inverse Document Frequency)\n",
    "\n",
    "**Problème avec TF** : Les mots fréquents comme \"le\", \"la\" ont des scores élevés mais peu d'importance.\n",
    "\n",
    "**Solution** : Pénaliser les mots qui apparaissent dans beaucoup de documents.\n",
    "\n",
    "**Formule** : TF-IDF(t, d) = TF(t, d) × IDF(t)\n",
    "\n",
    "Où IDF(t) = log(Nombre total de documents / Nombre de documents contenant t)\n",
    "### Exemple avec scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef0c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : ['chat' 'chien' 'croquettes' 'est' 'la' 'le' 'les' 'mange' 'petite'\n",
      " 'souris']\n",
      "\n",
      "Matrice TF-IDF :\n",
      "[[0.54935123 0.         0.         0.         0.41779577 0.41779577\n",
      "  0.         0.41779577 0.         0.41779577]\n",
      " [0.         0.49047908 0.49047908 0.         0.         0.37302199\n",
      "  0.49047908 0.37302199 0.         0.        ]\n",
      " [0.         0.         0.         0.5628291  0.42804604 0.\n",
      "  0.         0.         0.5628291  0.42804604]]\n",
      "\n",
      "           chat     chien  croquettes       est        la        le       les  \\\n",
      "Doc1  0.549351  0.000000    0.000000  0.000000  0.417796  0.417796  0.000000   \n",
      "Doc2  0.000000  0.490479    0.490479  0.000000  0.000000  0.373022  0.490479   \n",
      "Doc3  0.000000  0.000000    0.000000  0.562829  0.428046  0.000000  0.000000   \n",
      "\n",
      "         mange    petite    souris  \n",
      "Doc1  0.417796  0.000000  0.417796  \n",
      "Doc2  0.373022  0.000000  0.000000  \n",
      "Doc3  0.000000  0.562829  0.428046  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Nos documents\n",
    "documents = [\n",
    "    \"le chat mange la souris\",\n",
    "    \"le chien mange les croquettes\",\n",
    "    \"la souris est petite\"\n",
    "]\n",
    "\n",
    "# Créer le vectoriseur TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Afficher les mots du vocabulaire\n",
    "print(\"Vocabulaire :\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nMatrice TF-IDF :\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Créer un DataFrame pour mieux visualiser\n",
    "df_tfidf = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=['Doc1', 'Doc2', 'Doc3']\n",
    ")\n",
    "print(\"\\n\", df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8b793",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes\n",
    "\n",
    "### 2.1 Concept\n",
    "\n",
    "**Naive Bayes** est un algorithme de classification basé sur le théorème de Bayes.\n",
    "\n",
    "**Théorème de Bayes** : P(Classe|Document) = P(Document|Classe) × P(Classe) / P(Document)\n",
    "\n",
    "**\"Naive\"** : On suppose que les mots sont indépendants (hypothèse naïve mais efficace en pratique).\n",
    "\n",
    "### 2.2 Exemple simple : Classification de sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6243feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire : ['adoré' 'affreux' 'ai' 'bon' 'bravo' 'catastrophe' 'ce' 'déteste' 'est'\n",
      " 'excellent' 'film' 'horrible' 'je' 'magnifique' 'mauvais' 'nul' 'superbe'\n",
      " 'très']\n",
      "\n",
      "Matrice de comptage :\n",
      "[[1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0]]\n",
      "'film excellent' → POSITIF\n",
      "'film horrible' → NÉGATIF\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Dataset d'exemple : avis de films\n",
    "textes = [\n",
    "    \"ce film est excellent j'ai adoré\",\n",
    "    \"superbe film très bon\",\n",
    "    \"film magnifique bravo\",\n",
    "    \"film horrible très mauvais\",\n",
    "    \"nul je déteste ce film\",\n",
    "    \"catastrophe film affreux\"\n",
    "]\n",
    "\n",
    "# Labels : 1 = positif, 0 = négatif\n",
    "labels = [1, 1, 1, 0, 0, 0]\n",
    "\n",
    "# Étape 1 : Vectorisation (convertir texte en nombres)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(textes)\n",
    "\n",
    "print(\"Vocabulaire :\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nMatrice de comptage :\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Étape 2 : Entraîner le modèle\n",
    "model = MultinomialNB()\n",
    "model.fit(X, labels)\n",
    "\n",
    "# Étape 3 : Prédire de nouveaux textes\n",
    "nouveaux_textes = [\n",
    "    \"film excellent\",\n",
    "    \"film horrible\"\n",
    "]\n",
    "\n",
    "X_nouveaux = vectorizer.transform(nouveaux_textes)\n",
    "predictions = model.predict(X_nouveaux)\n",
    "\n",
    "for texte, pred in zip(nouveaux_textes, predictions):\n",
    "    sentiment = \"POSITIF\" if pred == 1 else \"NÉGATIF\"\n",
    "    print(f\"'{texte}' → {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c4350",
   "metadata": {},
   "source": [
    "### 2.3 Exemple plus réaliste : Classification de catégories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d10287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset : classification d'articles\n",
    "articles = [\n",
    "    \"le match de football était passionnant\",\n",
    "    \"le championnat de tennis commence demain\",\n",
    "    \"nouvelle victoire pour l'équipe de basketball\",\n",
    "    \"le nouveau smartphone est révolutionnaire\",\n",
    "    \"l'ordinateur portable est très performant\",\n",
    "    \"la tablette tactile a un bel écran\",\n",
    "    \"le gouvernement annonce une nouvelle loi\",\n",
    "    \"le président rencontre les députés\",\n",
    "    \"débat politique sur les réformes\"\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    \"sport\", \"sport\", \"sport\",\n",
    "    \"tech\", \"tech\", \"tech\",\n",
    "    \"politique\", \"politique\", \"politique\"\n",
    "]\n",
    "\n",
    "# Séparation train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    articles, categories, test_size=0.33, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e5a9515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision : 66.67%\n",
      "\n",
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   politique       0.50      1.00      0.67         1\n",
      "       sport       1.00      1.00      1.00         1\n",
      "        tech       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.50      0.67      0.56         3\n",
      "weighted avg       0.50      0.67      0.56         3\n",
      "\n",
      "'le joueur marque un but incroyable' → Catégorie : TECH\n",
      "'lancement du nouveau téléphone portable' → Catégorie : TECH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nessr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nessr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\nessr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Vectorisation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Entraînement\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_vec, y_train)\n",
    "\n",
    "# Évaluation\n",
    "predictions = classifier.predict(X_test_vec)\n",
    "print(f\"Précision : {accuracy_score(y_test, predictions):.2%}\")\n",
    "print(\"\\nRapport de classification :\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Prédire de nouveaux articles\n",
    "nouveaux_articles = [\n",
    "    \"le joueur marque un but incroyable\",\n",
    "    \"lancement du nouveau téléphone portable\"\n",
    "]\n",
    "X_nouveaux = vectorizer.transform(nouveaux_articles)\n",
    "predictions = classifier.predict(X_nouveaux)\n",
    "\n",
    "for article, categorie in zip(nouveaux_articles, predictions):\n",
    "    print(f\"'{article}' → Catégorie : {categorie.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f60b13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Exemple complet : Détecteur de spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74367f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de SMS (spam ou non)\n",
    "messages = [\n",
    "    \"Félicitations! Vous avez gagné 1000€! Cliquez ici\",\n",
    "    \"Promotion exceptionnelle! Offre limitée aujourd'hui seulement\",\n",
    "    \"URGENT: Votre compte sera fermé. Cliquez maintenant\",\n",
    "    \"Salut, on se voit ce soir pour le cinéma?\",\n",
    "    \"N'oublie pas la réunion de demain à 14h\",\n",
    "    \"Merci pour ton message, je te rappelle plus tard\",\n",
    "    \"100% GRATUIT! Téléchargez maintenant\",\n",
    "    \"Voulez-vous dîner ensemble samedi?\",\n",
    "    \"Rendez-vous chez le médecin confirmé pour lundi\",\n",
    "    \"GAGNEZ de l'argent facilement en travaillant de chez vous\"\n",
    "]\n",
    "# 1 = spam, 0 = non spam\n",
    "etiquettes = [1, 1, 1, 0, 0, 0, 1, 0, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b31da444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DÉTECTEUR DE SPAM ===\n",
      "\n",
      "Précision sur le test : 33.33%\n",
      "\n",
      "=== PRÉDICTIONS ===\n",
      "OK (58.4%) : 'Réunion annulée, on reporte à demain'\n",
      "SPAM (71.2%) : 'GAGNEZ 5000€ SANS EFFORT cliquez ICI'\n",
      "OK (52.0%) : 'Tu viens au restaurant ce midi?'\n"
     ]
    }
   ],
   "source": [
    "# Pipeline complète\n",
    "print(\"=== DÉTECTEUR DE SPAM ===\\n\")\n",
    "\n",
    "# 1. Vectorisation avec TF-IDF\n",
    "vectorizer = TfidfVectorizer(lowercase=True, max_features=50)\n",
    "X = vectorizer.fit_transform(messages)\n",
    "\n",
    "# 2. Division train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, etiquettes, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Entraînement du modèle\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Évaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Précision sur le test : {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "\n",
    "# 5. Prédictions sur de nouveaux messages\n",
    "nouveaux_messages = [\n",
    "    \"Réunion annulée, on reporte à demain\",\n",
    "    \"GAGNEZ 5000€ SANS EFFORT cliquez ICI\",\n",
    "    \"Tu viens au restaurant ce midi?\"\n",
    "]\n",
    "\n",
    "print(\"=== PRÉDICTIONS ===\")\n",
    "X_nouveaux = vectorizer.transform(nouveaux_messages)\n",
    "predictions = model.predict(X_nouveaux)\n",
    "probas = model.predict_proba(X_nouveaux)\n",
    "\n",
    "for msg, pred, proba in zip(nouveaux_messages, predictions, probas):\n",
    "    type_msg = \"SPAM\" if pred == 1 else \"OK\"\n",
    "    confiance = proba[pred] * 100\n",
    "    print(f\"{type_msg} ({confiance:.1f}%) : '{msg}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20074a",
   "metadata": {},
   "source": [
    "# Les N-grammes\n",
    "\n",
    "## Définition\n",
    "\n",
    "Un **n-gramme** est une séquence de **n** mots consécutifs dans un texte.\n",
    "\n",
    "- **1-gramme** (unigramme) = 1 mot\n",
    "- **2-grammes** (bigramme) = 2 mots consécutifs\n",
    "- **3-grammes** (trigramme) = 3 mots consécutifs\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab34030",
   "metadata": {},
   "source": [
    "## Exemple concret\n",
    "\n",
    "Prenons la phrase : **\"le chat mange la souris\"**\n",
    "\n",
    "### Unigrammes (1-gramme)\n",
    "```\n",
    "[\"le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
    "```\n",
    "\n",
    "### Bigrammes (2-grammes)\n",
    "```\n",
    "[\"le chat\", \"chat mange\", \"mange la\", \"la souris\"]\n",
    "```\n",
    "\n",
    "### Trigrammes (3-grammes)\n",
    "```\n",
    "[\"le chat mange\", \"chat mange la\", \"mange la souris\"]\n",
    "```\n",
    "\n",
    "Les n-grammes capturent le **contexte** et les **expressions** :\n",
    "\n",
    "**Sans n-grammes** : \"pas\" et \"bon\" sont séparés\n",
    "**Avec bigrammes** : \"pas bon\" est reconnu comme une expression négative\n",
    "\n",
    "## Exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3a5c834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== UNIGRAMMES ===\n",
      "Vocabulaire : ['bon' 'ce' 'est' 'excellent' 'film' 'le' 'pas' 'très']\n",
      "\n",
      "=== BIGRAMMES ===\n",
      "Vocabulaire : ['ce film' 'est excellent' 'est pas' 'est très' 'film est' 'le film'\n",
      " 'pas bon' 'très bon']\n",
      "\n",
      "=== UNIGRAMMES + BIGRAMMES ===\n",
      "Vocabulaire : ['bon' 'ce' 'ce film' 'est' 'est excellent' 'est pas' 'est très'\n",
      " 'excellent' 'film' 'film est' 'le' 'le film' 'pas' 'pas bon' 'très'\n",
      " 'très bon']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\n",
    "    \"le film est très bon\",\n",
    "    \"le film est pas bon\",\n",
    "    \"ce film est excellent\"\n",
    "]\n",
    "\n",
    "# Unigrammes seulement (par défaut)\n",
    "vectorizer_uni = CountVectorizer()\n",
    "X_uni = vectorizer_uni.fit_transform(documents)\n",
    "\n",
    "print(\"=== UNIGRAMMES ===\")\n",
    "print(\"Vocabulaire :\", vectorizer_uni.get_feature_names_out())\n",
    "print()\n",
    "\n",
    "# Bigrammes seulement\n",
    "vectorizer_bi = CountVectorizer(ngram_range=(2, 2))\n",
    "X_bi = vectorizer_bi.fit_transform(documents)\n",
    "\n",
    "print(\"=== BIGRAMMES ===\")\n",
    "print(\"Vocabulaire :\", vectorizer_bi.get_feature_names_out())\n",
    "print()\n",
    "\n",
    "# Unigrammes + Bigrammes\n",
    "vectorizer_mix = CountVectorizer(ngram_range=(1, 2))\n",
    "X_mix = vectorizer_mix.fit_transform(documents)\n",
    "\n",
    "print(\"=== UNIGRAMMES + BIGRAMMES ===\")\n",
    "print(\"Vocabulaire :\", vectorizer_mix.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1d2c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Impact sur la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d535039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Avis de restaurant\n",
    "avis = [\n",
    "    \"la nourriture est bonne mais le service est mauvais\",\n",
    "    \"excellent restaurant le service est parfait\",\n",
    "    \"nourriture pas bonne et service lent\",\n",
    "    \"très bon restaurant service rapide\",\n",
    "    \"décevant la nourriture est pas terrible\",\n",
    "    \"parfait tout était bon le service aussi\"\n",
    "]\n",
    "\n",
    "sentiments = [0, 1, 0, 1, 0, 1]  # 0=négatif, 1=positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9e32ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPARAISON UNIGRAMMES vs BIGRAMMES ===\n",
      "\n",
      "AVEC UNIGRAMMES SEULEMENT :\n",
      "  'nourriture pas bonne' → NÉGATIF\n",
      "  'service très bon' → POSITIF\n"
     ]
    }
   ],
   "source": [
    "print(\"=== COMPARAISON UNIGRAMMES vs BIGRAMMES ===\\n\")\n",
    "\n",
    "# Test 1 : Unigrammes seulement\n",
    "vectorizer1 = TfidfVectorizer(ngram_range=(1, 1))\n",
    "X1 = vectorizer1.fit_transform(avis)\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(X1, sentiments)\n",
    "\n",
    "test_avis = [\"nourriture pas bonne\", \"service très bon\"]\n",
    "X_test1 = vectorizer1.transform(test_avis)\n",
    "pred1 = model1.predict(X_test1)\n",
    "\n",
    "print(\"AVEC UNIGRAMMES SEULEMENT :\")\n",
    "for avis_test, pred in zip(test_avis, pred1):\n",
    "    print(f\"  '{avis_test}' → {'POSITIF' if pred == 1 else 'NÉGATIF'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0cd71eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AVEC UNIGRAMMES + BIGRAMMES :\n",
      "  'nourriture pas bonne' → NÉGATIF\n",
      "  'service très bon' → POSITIF\n"
     ]
    }
   ],
   "source": [
    "# Test 2 : Unigrammes + Bigrammes\n",
    "vectorizer2 = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X2 = vectorizer2.fit_transform(avis)\n",
    "model2 = MultinomialNB()\n",
    "model2.fit(X2, sentiments)\n",
    "\n",
    "X_test2 = vectorizer2.transform(test_avis)\n",
    "pred2 = model2.predict(X_test2)\n",
    "\n",
    "print(\"\\nAVEC UNIGRAMMES + BIGRAMMES :\")\n",
    "for avis_test, pred in zip(test_avis, pred2):\n",
    "    print(f\"  '{avis_test}' → {'POSITIF' if pred == 1 else 'NÉGATIF'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aa43122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BIGRAMMES IMPORTANTS DÉTECTÉS ===\n",
      "Exemples : ['bon le', 'bon restaurant', 'bonne et', 'bonne mais', 'décevant la', 'est bonne', 'est mauvais', 'est parfait', 'est pas', 'et service']\n"
     ]
    }
   ],
   "source": [
    "# Montrer les bigrammes détectés\n",
    "print(\"\\n=== BIGRAMMES IMPORTANTS DÉTECTÉS ===\")\n",
    "bigrammes = [mot for mot in vectorizer2.get_feature_names_out() if ' ' in mot]\n",
    "print(f\"Exemples : {bigrammes[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd267188",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exemple 4 : Cas d'usage réel - Détection d'expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08d044d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expressions idiomatiques et négations\n",
    "textes = [\n",
    "    \"ce n'est pas mal\",          # Négation importante\n",
    "    \"pas du tout satisfait\",      # Double négation\n",
    "    \"machine à laver\",            # Expression figée\n",
    "    \"pomme de terre\",             # Expression figée\n",
    "    \"tout à fait d'accord\",       # Expression positive\n",
    "    \"pas d'accord du tout\"        # Expression négative\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36253db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIGRAMMES (perd le contexte) :\n",
      "['accord' 'ce' 'de' 'du' 'est' 'fait' 'laver' 'machine' 'mal' 'pas'\n",
      " 'pomme' 'satisfait' 'terre' 'tout']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Avec unigrammes : perd le sens\n",
    "vectorizer_uni = CountVectorizer(ngram_range=(1, 1))\n",
    "X_uni = vectorizer_uni.fit_transform(textes)\n",
    "\n",
    "print(\"UNIGRAMMES (perd le contexte) :\")\n",
    "print(vectorizer_uni.get_feature_names_out())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38136be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVEC N-GRAMMES (capture les expressions) :\n",
      "['accord du', 'accord du tout', 'ce est', 'ce est pas', 'de terre', 'du tout', 'du tout satisfait', 'est pas', 'est pas mal', 'fait accord', 'machine laver', 'pas accord', 'pas accord du', 'pas du', 'pas du tout', 'pas mal', 'pomme de', 'pomme de terre', 'tout fait', 'tout fait accord', 'tout satisfait']\n"
     ]
    }
   ],
   "source": [
    "# Avec bigrammes et trigrammes : capture le sens\n",
    "vectorizer_multi = CountVectorizer(ngram_range=(1, 3))\n",
    "X_multi = vectorizer_multi.fit_transform(textes)\n",
    "\n",
    "print(\"AVEC N-GRAMMES (capture les expressions) :\")\n",
    "ngrams_detectes = [mot for mot in vectorizer_multi.get_feature_names_out() if ' ' in mot]\n",
    "print(ngrams_detectes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3106e041",
   "metadata": {},
   "source": [
    "## Quand utiliser les n-grammes ?\n",
    "\n",
    "### Utiliser les n-grammes quand :\n",
    "\n",
    "1. **Négations** : \"pas bon\", \"n'est pas\", \"ne... pas\"\n",
    "2. **Expressions figées** : \"machine à laver\", \"arc en ciel\"\n",
    "3. **Noms composés** : \"New York\", \"intelligence artificielle\"\n",
    "4. **Expressions de sentiment** : \"très bien\", \"pas mal\", \"tout à fait\"\n",
    "5. **Contexte important** : \"service client\", \"retour produit\"\n",
    "\n",
    "### Inconvénients des n-grammes :\n",
    "\n",
    "1. **Augmente la dimensionnalité** : beaucoup plus de features\n",
    "2. **Risque de surapprentissage** : avec peu de données\n",
    "3. **Plus lent** : calculs plus complexes\n",
    "4. **Sparsité** : matrices très creuses\n",
    "\n",
    "## Recommandations pratiques\n",
    "\n",
    "```python\n",
    "# Configuration typique recommandée\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Unigrammes + Bigrammes\n",
    "    max_features=5000,       # Limiter le nombre de features\n",
    "    min_df=2                 # Ignorer les mots trop rares\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1715e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba475cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad5a8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
